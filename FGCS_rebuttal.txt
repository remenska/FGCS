We value the effort invested by the reviewers in assessing our paper.
A list of changes for the raised points:
-------------
* [Introduction clearly describes one of the main interests of the paper: a methodology for building models (in the model-checking sense) for a legacy system. On the other hands, it fails to introduce the second contribution of the paper, which is a methodology to actually performing the check...]

> Introduction has been modified to introduce these aspects, paragraph:
[Once the model is in place, the toolset can automatically check for general system properties, such as deadlocks. We show how to formulate and check application-specific properties... 
This idea is simple but flexible and generic enough to describe many temporal properties of concurrent systems.]
-------------
* [However, for a reader not familiar with the history of the EGEE/EGI/WLCG grid, some information is missing. Mainly, Dirac appears as a standalone system, and the underlying software and hardware infrastructure is not described.
- The paper should explain the relation of Dirac with the gLite middleware, and provide a reference to  glite (eg E. Laure, S.M. Fisher, A. Frohner, C. Grandi, P. Kunszt, et al. Programming the Grid with gLite. Computational Methods in Science and Technology, 12(1):33-45, 2006.).]

> This explanation has been added in Section 2.2, paragraph:
[Originally, the DIRAC WMS submitted pilot jobs using the native gLite middleware tools, but this method was shown to be suboptimal. The gLite resource brokers... making sites compete with each other for jobs and have a faster job turnaround.]
> gLite reference also added
-------------
* [The paper should give figures showing the size and scale of distribution of the hardware infrastructure (number of sites, of CPUs or cores, storage capacity etc.) accessible to DIRAC.]
> Paragraph added:
[It provides a concurrent use of over 60K CPUs and 10M file replicas distributed over tens of grid sites, ....About 10PB of data is stored and visible in the LHCb file catalogs, with daily data transfers between storage units reaching 3Gbps.] 
> Figures 1 and 2 added for illustration.
-------------
* [Considering the SMS, a state  machine analogous to the one for WMS would improve the readability of section 2.3.]
> Figure 7 added.
-------------
* [ Other approaches of model checking (eg deductive verification) could have been discussed. Also, the choice of model checking versus trace mining (a very active research area) should be justified, as both Dirac and gLite services generate copious logs that were not exploited for diagnosis. Overall, a Related Work section is necessary.]
> Related work was addressed by all reviewers, so we dedicated a separate Section 5 for this.

* [Section 4 is also very pedagogical: it follows the real-world checking process, including preliminary visualization. However, figures 7 should be provided with a better resolution and better explained ... Figure 8 should probably printed in color, or entirely removed. If kept, the "suspicious transition" should be pointed in the figure.]
> Figure 7 (now 10) is now with a better resolution (background circles simply indicate which portion of the entire state space belongs to this self-transitions. We found it un-necessary to bother the reader with this detail). Figure 8 (now 11): suspicious transition is clearly pointed at, now.
-------------
* [Section 4.3 demonstrates the efficiency of the approach, on the smallest of the two state machine. On the other hand, the approach fails on the WMS, due to combinatorial explosion.  The strategy to cope with this issue is presented as a "trick", and should be better related to the relevant concepts in the literature, in particular to testing.]
> We briefly relate this in Section 5. Related Work [This component resembles our monitoring process automaton used to cope with state space explosion during model checking of DIRACâ€™s WMS system]
-------------
* [The authors motivate the problem of lost efficiency of resource usage and manpower. Are there any statistics showing actual loss of efficiency? How often the DIRAC system get into inconsistent state?]
> Added in Introduction: [Depending on the frequency of certain heavier data (re)processing campaigns throughout the year, the occurrence of such inconsistencies can be up to several reported cases per month..., compared to a temporary fix, like manipulating database records or restarting processes manually]
-------------
* [The application of model checking on DIRAC system and finding race conditions is useful. However, the extendability of this approach for other systems is not clear to me. Can these approaches be applied on other systems? What changes need to be done?]
> We attempt to tackle this in Section 6. Conclusions. A few more lines of explanation are added in: [It is observed that many large concurrent/distributed systems are essentially parametrized, meaning that they contain components or process descriptions which are behaviorally similar...and we believe that the combination of code slicing and abstraction rules of thumb presented in Section 3 can be helpful in this direction.]
-------------
* Reviewer #4's comments again raise the issues of applicability of the approach to other systems, and in depth discussion of related work. (covered above)
-------------
* [Some discussion on the complexity and/or time requirements of using the model would be interesting to understand the usefulness of the model]
> This is difficult to generalize for any model, but we provide statistics on the state space generation and model checking times in section 4.Analysis and Issues.

Best regards,
Daniela

